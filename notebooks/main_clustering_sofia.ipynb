{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f031a7b7",
   "metadata": {},
   "source": [
    "# Task 2: Cluster product categories into just 4-6 of them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfdbffa",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Suppose you have reviews for these products:\n",
    "\n",
    "- Kindle, iPad, Kobo eReader ‚Üí cluster: **E-Readers/Tablets**\n",
    "\n",
    "- Wireless Mouse, Keyboard ‚Üí cluster: **Computer Accessories**\n",
    "\n",
    "- Blender, Toaster ‚Üí cluster: **Kitchen Appliances**\n",
    "\n",
    "### Process: \n",
    "\n",
    "Clean text ‚Üí convert to embeddings ‚Üí cluster ‚Üí analyze\n",
    "\n",
    "### Goal: \n",
    "Reduce complexity of product categories to 4‚Äì6 meaningful clusters, making it easier to summarize reviews and recommend top products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cbe1cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined shape: (67992, 27)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file1 = \"../data/1429_1.csv\"\n",
    "file2 = \"../data/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv\"\n",
    "file3 = \"../data/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products.csv\"\n",
    "\n",
    "# Load your three CSV files\n",
    "df1 = pd.read_csv(file1, low_memory=False)\n",
    "df2 = pd.read_csv(file2, low_memory=False)\n",
    "df3 = pd.read_csv(file3, low_memory=False)\n",
    "\n",
    "# Combine the datasets of the 3 files\n",
    "df_combined = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "print(\"Combined shape:\", df_combined.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c62d70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after removing duplicates: (67897, 27)\n"
     ]
    }
   ],
   "source": [
    "# Remove dublicates\n",
    "df_combined.drop_duplicates(inplace=True)\n",
    "print(\"Shape after removing duplicates:\", df_combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3c8da48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index\n",
    "df_combined.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf83f83",
   "metadata": {},
   "source": [
    "### Clean code for text-preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676c5394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        reviews.text  \\\n",
      "0  This product so far has not disappointed. My c...   \n",
      "1  great for beginner or experienced person. Boug...   \n",
      "2  Inexpensive tablet for him to use and learn on...   \n",
      "3  I've had my Fire HD 8 two weeks now and I love...   \n",
      "4  I bought this for my grand daughter when she c...   \n",
      "\n",
      "                                          clean_text  \n",
      "0  product far disappointed child love use like a...  \n",
      "1  great beginner experienced person bought gift ...  \n",
      "2  inexpensive tablet use learn step nabi thrille...  \n",
      "3  ive fire hd two week love tablet great valuewe...  \n",
      "4  bought grand daughter come visit set user ente...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize lemmatizer and stopwords list\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove stopwords and lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words]\n",
    "    \n",
    "    # Join tokens back to string\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Apply cleaning to the 'reviews.text' column of df_combined\n",
    "df_combined['clean_text'] = df_combined['reviews.text'].apply(clean_text)\n",
    "\n",
    "# Optional: inspect results\n",
    "print(df_combined[['reviews.text', 'clean_text']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44a085ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values (%):\n",
      " reviews.userProvince    100.000000\n",
      "reviews.userCity        100.000000\n",
      "reviews.didPurchase      99.985272\n",
      "reviews.id               99.895430\n",
      "reviews.dateAdded        63.057278\n",
      "sourceURLs               51.047911\n",
      "manufacturerNumber       51.047911\n",
      "imageURLs                51.047911\n",
      "primaryCategories        51.047911\n",
      "dateAdded                51.047911\n",
      "dateUpdated              51.047911\n",
      "reviews.doRecommend      18.910998\n",
      "reviews.numHelpful       18.772553\n",
      "name                      9.956257\n",
      "reviews.date              0.057440\n",
      "reviews.rating            0.048603\n",
      "reviews.title             0.027984\n",
      "reviews.username          0.019147\n",
      "asins                     0.002946\n",
      "reviews.text              0.001473\n",
      "id                        0.000000\n",
      "reviews.sourceURLs        0.000000\n",
      "reviews.dateSeen          0.000000\n",
      "manufacturer              0.000000\n",
      "keys                      0.000000\n",
      "categories                0.000000\n",
      "brand                     0.000000\n",
      "clean_text                0.000000\n"
     ]
    }
   ],
   "source": [
    "#df_combined.info()\n",
    "\n",
    "missing_percent = (df_combined.isnull().sum() / len(df_combined) * 100).sort_values(ascending=False)\n",
    "print(\"\\nMissing values (%):\\n\", missing_percent.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1690a1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Created folder: ../outputs\n",
      "‚úÖ DataFrame successfully exported to: ../outputs/combined_documents_cleaned_sofia.csv\n"
     ]
    }
   ],
   "source": [
    "# Export the cleaned document\n",
    "import os\n",
    "\n",
    "output_dir = \"../outputs\"\n",
    "output_file = os.path.join(output_dir, \"combined_documents_cleaned_sofia.csv\")\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"üìÅ Created folder: {output_dir}\")\n",
    "else:\n",
    "    print(f\"üìÅ Folder already exists: {output_dir}\")\n",
    "\n",
    "# Export the DataFrame to CSV\n",
    "df_combined.to_csv(output_file, index=False)\n",
    "print(f\"‚úÖ DataFrame successfully exported to: {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvSofia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
